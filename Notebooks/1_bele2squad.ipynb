{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Malik Altakrori, PhD\n",
    "# IBM Research\n",
    "# malik.altakrori@ibm.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook 1\n",
    "Prep the Belebele dataset, and put it in the SQuAD format to use it with PrimeQA toolkit from IBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset   \n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = \"<Provide the abs path to the repo>\"\n",
    "data_path = \"Data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read both files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_data_en = load_dataset(\"csv\", data_files=os.path.join(root_folder, data_path, \"Annotated/416_Annotated_En.csv\"), split=\"train\")\n",
    "All_data_ar = load_dataset(\"csv\", data_files=os.path.join(root_folder, data_path, \"Annotated/416_Annotated_Ar.csv\"), split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"We have {len(All_data_ar)} Arabic Questions, and:\")\n",
    "print(f\"We have {len(All_data_en)} English Questions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove rows with X annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of rows English BEFORE automatic filtering: {len(All_data_en)}\")\n",
    "print(f\"Total number of rows Arabic  BEFORE automatic filtering: {len(All_data_ar)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_data_en = All_data_en.filter(lambda example: example['Span']!='X')\n",
    "All_data_ar = All_data_ar.filter(lambda example: example['Annotated Passage']!='X')\n",
    "\n",
    "print(f\"Total number of rows English AFTER automatic filtering: {len(All_data_en)}\")\n",
    "print(f\"Total number of rows Arabic  AFTER automatic filtering: {len(All_data_ar)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_data_en, All_data_ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the offset (where the answer span start from)\n",
    "# with a test example\n",
    "\n",
    "def find_offset_En(example):\n",
    "    spn_start = example['Span'].find('$!') \n",
    "    spn_end = example['Span'].find('!$')\n",
    "    answer = example['Span'][spn_start+2:spn_end]\n",
    "    example['Span_en'] = answer\n",
    "    example['offset_en'] = spn_start\n",
    "    \n",
    "    if spn_start == -1 or spn_end == -1:\n",
    "        print(example)\n",
    "    return example\n",
    "\n",
    "example = {\"flores_passage\": \"The American plan relied on launching coordinated attacks from three different directions. General John Cadwalder would launch a diversionary attack against the British garrison at Bordentown, in order to block off any reinforcements. General James Ewing would take 700 militia across the river at Trenton Ferry, seize the bridge over the Assunpink Creek and prevent any enemy troops from escaping. The main assault force of 2,400 men would cross the river nine miles north of Trenton, and then split into two groups, one under Greene and one under Sullivan, in order to launch a pre-dawn attack.\",\n",
    "           \"Span\": \"The American plan relied on launching coordinated attacks from three different directions. General John Cadwalder would launch a diversionary attack against the British garrison at $!Bordentown!$, in order to block off any reinforcements. General James Ewing would take 700 militia across the river at Trenton Ferry, seize the bridge over the Assunpink Creek and prevent any enemy troops from escaping. The main assault force of 2,400 men would cross the river nine miles north of Trenton, and then split into two groups, one under Greene and one under Sullivan, in order to launch a pre-dawn attack.\",\n",
    "            \"Arabic passage\":\"وطالب المستعمرون، الذين شاهدوا هذا النشاط، بتعزيزات. ضمّت تعزيزات المواقع الأمامية كتيبتي نيوهامبشير الأولى والثالثة في صفوفهما 200 من الرجال يقودهم العقيد چون ستارك والعقيد چيمس ريد (والذين أصبحا بعدها جنرالين). تمركز رجال ستارك في مواقع بطول السياج من الطرف الشمالي لمكان المستعمر. عندما فتح المدّ المنخفض فجوة على طول النهر الغامض على طول الجانب الشمالي الشرقي من شبه الجزيرة، مدّوا السياج سريعاً بجدار حجري قصير إلى الشمال ينتهي عند حافة المياه على شاطئ صغير. وضع جريدلي أو ستارك وتدًا على بعد حوالي 100 قدم (30 مترًا) أمام السياج وأمر ألا يطلق أحد النار حتى يعبره ضباط الجيش.\",\n",
    "            \"Pre-Annotated Arabic Passage\": \"وطالب المستعمرون، الذين شاهدوا هذا النشاط، بتعزيزات. ضمّت تعزيزات المواقع الأمامية كتيبتي نيوهامبشير الأولى والثالثة في صفوفهما 200 من الرجال يقودهم العقيد چون ستارك والعقيد چيمس ريد (والذين أصبحا بعدها جنرالين). تمركز رجال ستارك في $!مواقع بطول السياج من الطرف الشمالي لمكان المستعمر.!$ عندما فتح المدّ المنخفض فجوة على طول النهر الغامض على طول الجانب الشمالي الشرقي من شبه الجزيرة، مدّوا السياج سريعاً بجدار حجري قصير إلى الشمال ينتهي عند حافة المياه على شاطئ صغير. وضع جريدلي أو ستارك وتدًا على بعد حوالي 100 قدم (30 مترًا) أمام السياج وأمر ألا يطلق أحد النار حتى يعبره ضباط الجيش.\"\n",
    "           }\n",
    "\n",
    "find_offset_En(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_offset_Ar(example):\n",
    "\n",
    "    spn_start = example['Annotated Passage'].find('$!') \n",
    "    spn_end = example['Annotated Passage'].find('!$')\n",
    "    if spn_end<spn_start:\n",
    "        print(\"oops\")\n",
    "        print(example)\n",
    "        temp = spn_end\n",
    "        spn_end = spn_start\n",
    "        spn_start = temp\n",
    "    example['Span_ar'] = example['Annotated Passage'][spn_start+2:spn_end]\n",
    "    example['offset_ar'] = spn_start\n",
    "                               \n",
    "    if spn_start == -1 or spn_end == -1:\n",
    "        print(example)\n",
    "    return example\n",
    "\n",
    "example = {\"flores_passage\": \"The Colonists, seeing this activity, had also called for reinforcements. Troops reinforcing the forward positions included the 1st and 3rd New Hampshire regiments of 200 men, under Colonels John Stark and James Reed (both later became generals). Stark's men took positions along the fence on the north end of the Colonist's position. When low tide opened a gap along the Mystic River along the northeast of the peninsula, they quickly extended the fence with a short stone wall to the north ending at the water's edge on a small beach. Gridley or Stark placed a stake about 100 feet (30 m) in front of the fence and ordered that no one fire until the regulars passed it.\",\n",
    "           \"Span\": \"The Colonists, seeing this activity, had also called for reinforcements. Troops reinforcing the forward positions included the 1st and 3rd New Hampshire regiments of 200 men, under Colonels John Stark and James Reed (both later became generals). Stark's men took positions $!along the fence on the north end of the Colonist's position!$. When low tide opened a gap along the Mystic River along the northeast of the peninsula, they quickly extended the fence with a short stone wall to the north ending at the water's edge on a small beach. Gridley or Stark placed a stake about 100 feet (30 m) in front of the fence and ordered that no one fire until the regulars passed it.\",\n",
    "            \"Arabic passage\":\"وطالب المستعمرون، الذين شاهدوا هذا النشاط، بتعزيزات. ضمّت تعزيزات المواقع الأمامية كتيبتي نيوهامبشير الأولى والثالثة في صفوفهما 200 من الرجال يقودهم العقيد چون ستارك والعقيد چيمس ريد (والذين أصبحا بعدها جنرالين). تمركز رجال ستارك في مواقع بطول السياج من الطرف الشمالي لمكان المستعمر. عندما فتح المدّ المنخفض فجوة على طول النهر الغامض على طول الجانب الشمالي الشرقي من شبه الجزيرة، مدّوا السياج سريعاً بجدار حجري قصير إلى الشمال ينتهي عند حافة المياه على شاطئ صغير. وضع جريدلي أو ستارك وتدًا على بعد حوالي 100 قدم (30 مترًا) أمام السياج وأمر ألا يطلق أحد النار حتى يعبره ضباط الجيش.\",\n",
    "            \"Annotated Passage\": \"وطالب المستعمرون، الذين شاهدوا هذا النشاط، بتعزيزات. ضمّت تعزيزات المواقع الأمامية كتيبتي نيوهامبشير الأولى والثالثة في صفوفهما 200 من الرجال يقودهم العقيد چون ستارك والعقيد چيمس ريد (والذين أصبحا بعدها جنرالين). تمركز رجال ستارك في $!مواقع بطول السياج من الطرف الشمالي لمكان المستعمر.!$ عندما فتح المدّ المنخفض فجوة على طول النهر الغامض على طول الجانب الشمالي الشرقي من شبه الجزيرة، مدّوا السياج سريعاً بجدار حجري قصير إلى الشمال ينتهي عند حافة المياه على شاطئ صغير. وضع جريدلي أو ستارك وتدًا على بعد حوالي 100 قدم (30 مترًا) أمام السياج وأمر ألا يطلق أحد النار حتى يعبره ضباط الجيش.\"\n",
    "           }\n",
    "\n",
    "find_offset_Ar(example)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the offset function, and check that no example has a -1 (the sign of wrong annotation)\n",
    "test_ar = All_data_ar.map(find_offset_Ar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making sure whe dont have any wrong offsets\n",
    "test_ar.filter(lambda example: example['offset_ar'] == -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check: the filter will return the whol dataset\n",
    "test_ar = test_ar.filter(lambda example: example['offset_ar'] != -1) # Notice that we flipped the == to !=\n",
    "test_ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_en = All_data_en.map(find_offset_En)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test example\n",
    "ex = {'Annotator': 'Anon', 'link': 'https://en.wikibooks.org/wiki/Basic_Physics_of_Digital_Radiography/The_Basics', 'question_number': 2, 'flores_passage': 'The atom can be considered to be one of the fundamental building blocks of all matter. Its a very complex entity which consists, according to a simplified Bohr model, of a central nucleus orbited by electrons, somewhat similar to planets orbiting the sun - see Figure 1.1. The nucleus consists of two particles - neutrons and protons. Protons have a positive electric charge while neutrons have no charge. The electrons have a negative electric charge.', 'question': 'The nucleus is composed of which particles?', 'dialect': 'eng_Latn', 'Span': 'The atom can be considered to be one of the fundamental building blocks of all matter. Its a very complex entity which consists, according to a simplified Bohr model, of a central nucleus orbited by electrons, somewhat similar to planets orbiting the sun - see Figure 1.1. $!neutrons and protons!$. Protons have a positive electric charge while neutrons have no charge. The electrons have a negative electric charge.', 'Arabic passage': 'يُمكن اعتبار الذرة واحدة من وحدات البناء الأساسية لكل المواد. إنه كيان معقد للغاية يتكون، وفقًا لنموذج بور البسيط، من نواة مركزية تدور حولها الإلكترونات، تشبه إلى حد ما الكواكب التي تدور حول الشمس - انظر الشكل 1.1. تتكون النواة من جسيمين - النيوترونات والبروتونات. للبروتونات شحنة كهربية موجبة، بينما النيوترونات ليس لها شحنة، أما الإلكترونات فشحنتها سالبة.', 'Arabic Question': 'من أي جسيمات تتكون النواة؟', 'Pre-Annotated Arabic Passage': 'يُمكن اعتبار الذرة واحدة من وحدات البناء الأساسية لكل المواد. إنه كيان معقد للغاية يتكون، وفقًا لنموذج بور البسيط، من نواة مركزية تدور حولها الإلكترونات، تشبه إلى حد ما الكواكب التي تدور حول الشمس - انظر الشكل 1.1. تتكون النواة من جسيمين - $!النيوترونات والبروتونات.!$ للبروتونات شحنة كهربية موجبة، بينما النيوترونات ليس لها شحنة، أما الإلكترونات فشحنتها سالبة.', 'Span_en': 'neutrons and protons', 'offset_en': 273}\n",
    "ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_en.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_ar.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not really original, but fixed manually by the authors ... one problematic row at a time .. \n",
    "# Important: This is modified from the HF files\n",
    "original_data_path = \"Data/Belebele_original\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "map the dialectal questions to English and MSA passages\n",
    "\n",
    "# First, we load the original questions (URL issue solved manually)\n",
    "\n",
    "# The following URLs have commas in the URL field. They had to be removed manually:\n",
    "\n",
    "# * problematic questions: 73, 290, 291, 357, 358, 400, 401, 452, 453,473, 474, 479, 480, 491, 492, 515, 516,535, 569, 570, 571, 572\n",
    "\"\"\"\n",
    "\n",
    "for f_name in ['acm', 'apc', 'ars', 'ary', 'arz']:\n",
    "    Qs = {}\n",
    "    def map_dialect(example):\n",
    "        # try:\n",
    "        example[f'question_{f_name}_Arab'] = Qs[example['link']][example['question_number']]\n",
    "        # except:\n",
    "            # print()\n",
    "        return example\n",
    "\n",
    "    print(f\"Processing {f_name}\")\n",
    "    with open(os.path.join(root_folder, original_data_path, f\"{f_name}_Arab.jsonl\"), \"r\") as f:\n",
    "        dial_ar = f.readlines()\n",
    "        for line in dial_ar:\n",
    "        # for line in acm_ar:\n",
    "            \n",
    "            line = json.loads(line)\n",
    "            # print(line)        \n",
    "            \n",
    "            if line['link'] not in Qs.keys():\n",
    "                Qs[line['link']] = {}\n",
    "                Qs[line['link']][int(line[\"question_number\"])] = line[\"question\"]\n",
    "            else:\n",
    "                Qs[line['link']][int(line[\"question_number\"])] = line[\"question\"]\n",
    "                \n",
    "            # Qs[int(line[-1].strip())] = line[3]\n",
    "    \n",
    "    print(f\"Prcessing {f_name} for English\")\n",
    "    test_en = test_en.map(map_dialect)\n",
    "    print(f\"Prcessing {f_name} for Arabic\")\n",
    "    test_ar = test_ar.map(map_dialect)\n",
    "    \n",
    "    print(f\"Prcessing {f_name} is complete\")\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map MSA questions to the EN dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_name ='arb'\n",
    "Qs = {}\n",
    "def map_dialect(example):\n",
    "    example[f'question_{f_name}_Arab'] = Qs[example['link']][example['question_number']]\n",
    "\n",
    "    return example\n",
    "\n",
    "print(f\"Processing {f_name}\")\n",
    "with open(os.path.join(root_folder, original_data_path, f\"{f_name}_Arab.jsonl\"), \"r\") as f:\n",
    "    dial_ar = f.readlines()\n",
    "    for line in dial_ar:\n",
    "        line = json.loads(line)\n",
    "        \n",
    "        if line['link'] not in Qs.keys():\n",
    "            Qs[line['link']] = {}\n",
    "            Qs[line['link']][int(line[\"question_number\"])] = line[\"question\"]\n",
    "        else:\n",
    "            Qs[line['link']][int(line[\"question_number\"])] = line[\"question\"]\n",
    "\n",
    "test_en = test_en.map(map_dialect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map EN questions to the MSA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_name ='eng'\n",
    "Qs = {}\n",
    "\n",
    "print(f\"Processing {f_name}\")\n",
    "with open(os.path.join(root_folder, original_data_path, f\"{f_name}_Latn.jsonl\"), \"r\") as f:\n",
    "    Lang_en = f.readlines()\n",
    "    for line in Lang_en:\n",
    "        line = json.loads(line)\n",
    "\n",
    "        if line['link'] not in Qs.keys():\n",
    "            Qs[line['link']] = {}\n",
    "            Qs[line['link']][int(line[\"question_number\"])] = line[\"question\"]\n",
    "        else:\n",
    "            Qs[line['link']][int(line[\"question_number\"])] = line[\"question\"]\n",
    "\n",
    "def map_dialect(example):\n",
    "    example[f'question_{f_name}_Latn'] = Qs[example['link']][example['question_number']]\n",
    "\n",
    "    return example\n",
    "\n",
    "test_ar = test_ar.map(map_dialect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice how the columns increased: (ignore \"unnamed \" columns. They are artifcats of excel and won't affect the script)\n",
    "test_en, test_ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = test_en[0]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ar = test_ar[0]\n",
    "sample_ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example from En making sure that the passage[from offset (start index) to: span lenghth] matches the actual span answer\n",
    "sample['flores_passage'][sample['offset_en']:sample['offset_en']+len(sample['Span_en'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as above but for Ar\n",
    "sample_ar['Arabic passage'][sample_ar['offset_ar']:sample_ar['offset_ar']+len(sample_ar['Span_ar'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the same verification but on a sample from the end\n",
    "sample_end = test_en[-1]\n",
    "sample_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_end['flores_passage'][sample_end['offset_en']:sample_end['offset_en']+len(sample_end['Span_en'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_end_ar = test_ar[-2]\n",
    "sample_end_ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_end_ar['Arabic passage'][sample_end_ar['offset_ar']:sample_end_ar['offset_ar']+len(sample_end_ar['Span_ar'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_en.column_names[:10])\n",
    "print(test_en.column_names[10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting All questions into Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work on the original or the translated dialectal questions\n",
    "Translated = True\n",
    "\n",
    "if not Translated:\n",
    "    settings_folder = \"Settings\"\n",
    "else:\n",
    "    settings_folder = \"Settings_Translated\"\n",
    "\n",
    "print(f\"working with {settings_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting English (All)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create the home folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = os.path.join(root_folder, settings_folder, \"All\")\n",
    "output_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite_existing = True\n",
    "try:\n",
    "    os.makedirs(output_folder, exist_ok=overwrite_existing)\n",
    "except:\n",
    "    print(f\"WARNING: Folder {output_folder} exists, either delete manually, or set overwrite_existing to True\")\n",
    "    raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"En Passage  -- En Qs\")\n",
    "\n",
    "json_samples = []\n",
    "for example in test_en:\n",
    "    json_samples.append({\"context\": example['flores_passage'],\n",
    "                        \"question\": example[f'question'],\n",
    "                        \"id\": str(len(json_samples)),\n",
    "                        \"answers\": {\"text\": [example['Span_en']], \"answer_start\": [example['offset_en']]}\n",
    "                        })\n",
    "\n",
    "with open(os.path.join(output_folder, \"EN-P_EN-Q.jsonl\"), encoding=\"utf8\", mode=\"w\") as f:\n",
    "    f.write(\"\\n\".join([json.dumps(json_sample) for json_sample in json_samples]))\n",
    "\n",
    "print(\"=====================================\")\n",
    "print(\"En Passage  -- MSA Qs\")\n",
    "\n",
    "json_samples = []\n",
    "for example in test_en:\n",
    "    json_samples.append({\"context\": example['flores_passage'],\n",
    "                        \"question\": example[f'question_arb_Arab'],\n",
    "                        \"id\": str(len(json_samples)),\n",
    "                        \"answers\": {\"text\": [example['Span_en']], \"answer_start\": [example['offset_en']]}\n",
    "                        })\n",
    "\n",
    "with open(os.path.join(output_folder, \"EN-P_MSA-Q.jsonl\"), encoding=\"utf8\", mode=\"w\") as f:\n",
    "    f.write(\"\\n\".join([json.dumps(json_sample) for json_sample in json_samples]))\n",
    "\n",
    "print(\"=====================================\")\n",
    "\n",
    "print(\"En Passage  -- Dialect Qs\")\n",
    "\n",
    "for dialect in  ['question_acm_Arab',\n",
    "    'question_apc_Arab',\n",
    "    'question_ars_Arab',\n",
    "    'question_ary_Arab',\n",
    "    'question_arz_Arab']:\n",
    "    dial = dialect.split('_')[1]\n",
    "    print(f\"En Passage  -- {dial} Qs\")\n",
    "\n",
    "    json_samples = []\n",
    "    for example in test_en:\n",
    "        json_samples.append({\"context\": example['flores_passage'],\n",
    "                            \"question\": example[dialect],\n",
    "                            \"id\": str(len(json_samples)),\n",
    "                            \"answers\": {\"text\": [example['Span_en']], \"answer_start\": [example['offset_en']]}\n",
    "                            })\n",
    "\n",
    "    \n",
    "    with open(os.path.join(output_folder, f\"EN-P_{dial}-Q.jsonl\"), encoding=\"utf8\", mode=\"w\") as f:\n",
    "        f.write(\"\\n\".join([json.dumps(json_sample) for json_sample in json_samples]))\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Arabic (All)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ar Passage  -- En Qs\")\n",
    "\n",
    "json_samples = []\n",
    "for example in test_ar:\n",
    "    json_samples.append({\"context\": example['Arabic passage'],\n",
    "                        \"question\": example[f'question_eng_Latn'],\n",
    "                        \"id\": str(len(json_samples)),\n",
    "                        \"answers\": {\"text\": [example['Span_ar']], \"answer_start\": [example['offset_ar']]}\n",
    "                        })\n",
    "    \n",
    "with open(os.path.join(output_folder, \"MSA-P_EN-Q.jsonl\"), encoding=\"utf8\", mode=\"w\") as f:\n",
    "    f.write(\"\\n\".join([json.dumps(json_sample) for json_sample in json_samples]))\n",
    "\n",
    "print(\"=====================================\")\n",
    "print(\"Ar Passage  -- MSA Qs\")\n",
    "\n",
    "json_samples = []\n",
    "for example in test_ar:\n",
    "    json_samples.append({\"context\": example['Arabic passage'],\n",
    "                        \"question\": example[f'Arabic Question'],\n",
    "                        \"id\": str(len(json_samples)),\n",
    "                        \"answers\": {\"text\": [example['Span_ar']], \"answer_start\": [example['offset_ar']]}\n",
    "                        })\n",
    "    \n",
    "with open(os.path.join(output_folder, \"MSA-P_MSA-Q.jsonl\"), encoding=\"utf8\", mode=\"w\") as f:\n",
    "    f.write(\"\\n\".join([json.dumps(json_sample) for json_sample in json_samples]))\n",
    "\n",
    "print(\"=====================================\")\n",
    "for dialect in  ['question_acm_Arab',\n",
    "    'question_apc_Arab',\n",
    "    'question_ars_Arab',\n",
    "    'question_ary_Arab',\n",
    "    'question_arz_Arab']:\n",
    "    dial = dialect.split('_')[1]\n",
    "    print(f\"Ar Passage  -- {dial} Qs\")\n",
    "\n",
    "    json_samples = []\n",
    "    for example in test_ar:\n",
    "        json_samples.append({\"context\": example['Arabic passage'],\n",
    "                            \"question\": example[dialect],\n",
    "                            \"id\": str(len(json_samples)),\n",
    "                            \"answers\": {\"text\": [example['Span_ar']], \"answer_start\": [example['offset_ar']]}\n",
    "                            })\n",
    "        \n",
    "    with open(os.path.join(output_folder, f\"MSA-P_{dial}-Q.jsonl\"), encoding=\"utf8\", mode=\"w\") as f:\n",
    "        f.write(\"\\n\".join([json.dumps(json_sample) for json_sample in json_samples]))\n",
    "\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excluding Belebele questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ar_noBB = test_ar.filter(lambda example: example['Belebele problem?']== 'No')\n",
    "test_ar_noBB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_ar.num_rows-test_ar_noBB.num_rows,' was removed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_en_noBB = test_en.filter(lambda example: example['Belebele problem?']== 'No')\n",
    "print(test_en.num_rows-test_en_noBB.num_rows,' was removed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating another output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = os.path.join(root_folder, settings_folder, \"NoBB\")\n",
    "output_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite_existing = True\n",
    "try:\n",
    "    os.makedirs(output_folder, exist_ok=overwrite_existing)\n",
    "except:\n",
    "    print(f\"WARNING: Folder {output_folder} exists, either delete manually, or set overwrite_existing to True\")\n",
    "    raise ValueError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting English (NoBB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"En Passage  -- EN Qs\")\n",
    "json_samples = []\n",
    "for example in test_en_noBB:\n",
    "    json_samples.append({\"context\": example['flores_passage'],\n",
    "                        \"question\": example[f'question'],\n",
    "                        \"id\": str(len(json_samples)),\n",
    "                        \"answers\": {\"text\": [example['Span_en']], \"answer_start\": [example['offset_en']]}\n",
    "                        })\n",
    "    \n",
    "with open(os.path.join(output_folder, \"EN-P_EN-Q.jsonl\"), encoding=\"utf8\", mode=\"w\") as f:\n",
    "    f.write(\"\\n\".join([json.dumps(json_sample) for json_sample in json_samples]))\n",
    "\n",
    "print(\"=====================================\")\n",
    "print(\"En Passage  -- MSA Qs\")\n",
    "json_samples = []\n",
    "for example in test_en_noBB:\n",
    "    json_samples.append({\"context\": example['flores_passage'],\n",
    "                        \"question\": example[f'question_arb_Arab'],\n",
    "                        \"id\": str(len(json_samples)),\n",
    "                        \"answers\": {\"text\": [example['Span_en']], \"answer_start\": [example['offset_en']]}\n",
    "                        })\n",
    "    \n",
    "with open(os.path.join(output_folder, \"EN-P_MSA-Q.jsonl\"), encoding=\"utf8\", mode=\"w\") as f:\n",
    "    f.write(\"\\n\".join([json.dumps(json_sample) for json_sample in json_samples]))\n",
    "\n",
    "print(\"=====================================\")\n",
    "print(\"En Passage  -- Dialectal Qs\")\n",
    "for dialect in  ['question_acm_Arab',\n",
    "    'question_apc_Arab',\n",
    "    'question_ars_Arab',\n",
    "    'question_ary_Arab',\n",
    "    'question_arz_Arab']:\n",
    "    dial = dialect.split('_')[1]\n",
    "    print(f\"En Passage  -- {dial} Qs\")\n",
    "\n",
    "    json_samples = []\n",
    "    for example in test_en_noBB:\n",
    "        json_samples.append({\"context\": example['flores_passage'],\n",
    "                            \"question\": example[dialect],\n",
    "                            \"id\": str(len(json_samples)),\n",
    "                            \"answers\": {\"text\": [example['Span_en']], \"answer_start\": [example['offset_en']]}\n",
    "                            })\n",
    "        \n",
    "    with open(os.path.join(output_folder, f\"EN-P_{dial}-Q.jsonl\"), encoding=\"utf8\", mode=\"w\") as f:\n",
    "        f.write(\"\\n\".join([json.dumps(json_sample) for json_sample in json_samples]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Arabic (NoBB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ar Passage  -- EN Qs\")\n",
    "json_samples = []\n",
    "for example in test_ar_noBB:\n",
    "    json_samples.append({\"context\": example['Arabic passage'],\n",
    "                        \"question\": example[f'question_eng_Latn'],\n",
    "                        \"id\": str(len(json_samples)),\n",
    "                        \"answers\": {\"text\": [example['Span_ar']], \"answer_start\": [example['offset_ar']]}\n",
    "                        })\n",
    "    \n",
    "with open(os.path.join(output_folder, \"MSA-P_EN-Q.jsonl\"), encoding=\"utf8\", mode=\"w\") as f:\n",
    "    f.write(\"\\n\".join([json.dumps(json_sample) for json_sample in json_samples]))\n",
    "\n",
    "print(\"=====================================\")\n",
    "print(\"Ar Passage  -- MSA Qs\")\n",
    "json_samples = []\n",
    "for example in test_ar_noBB:\n",
    "    json_samples.append({\"context\": example['Arabic passage'],\n",
    "                        \"question\": example[f'Arabic Question'],\n",
    "                        \"id\": str(len(json_samples)),\n",
    "                        \"answers\": {\"text\": [example['Span_ar']], \"answer_start\": [example['offset_ar']]}\n",
    "                        })\n",
    "    \n",
    "with open(os.path.join(output_folder, \"MSA-P_MSA-Q.jsonl\"), encoding=\"utf8\", mode=\"w\") as f:\n",
    "    f.write(\"\\n\".join([json.dumps(json_sample) for json_sample in json_samples]))\n",
    "\n",
    "print(\"=====================================\")\n",
    "print(\"Ar Passage  -- Dialectal Qs\")\n",
    "for dialect in  ['question_acm_Arab',\n",
    "    'question_apc_Arab',\n",
    "    'question_ars_Arab',\n",
    "    'question_ary_Arab',\n",
    "    'question_arz_Arab']:\n",
    "    dial = dialect.split('_')[1]\n",
    "    print(f\"Ar Passage  -- {dial} Qs\")\n",
    "\n",
    "    json_samples = []\n",
    "    for example in test_ar_noBB:\n",
    "        json_samples.append({\"context\": example['Arabic passage'],\n",
    "                            \"question\": example[dialect],\n",
    "                            \"id\": str(len(json_samples)),\n",
    "                            \"answers\": {\"text\": [example['Span_ar']], \"answer_start\": [example['offset_ar']]}\n",
    "                            })\n",
    "        \n",
    "    with open(os.path.join(output_folder, f\"MSA-P_{dial}-Q.jsonl\"), encoding=\"utf8\", mode=\"w\") as f:\n",
    "        f.write(\"\\n\".join([json.dumps(json_sample) for json_sample in json_samples]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to notebook 2 to prepare the experiments!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "primeqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
