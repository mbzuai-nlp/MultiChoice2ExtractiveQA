{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Malik Altakrori, PhD\n",
    "# IBM Research\n",
    "# malik.altakrori@ibm.com"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "This notebook extracts the results from the various files\n",
    "This is the last step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = \"<Provide the abs path to the repo>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dialect code name\n",
    "# https://github.com/facebookresearch/belebele\n",
    "\n",
    "dialects = {\n",
    "    \"en\" : \"En\",\n",
    "    \"msa\" : \"MSA\",\n",
    "    'apc': \"Levantine\",\n",
    "    'arz': \"Egyptian\",\n",
    "    'acm': \"Iraqi\",\n",
    "    'ary': \"Moroccan\",\n",
    "    'ars': \"Gulf\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose from: Results or Results_Translated\n",
    "\n",
    "# results_folder = \"Results\"  \n",
    "# results_folder = \"Results_Translated\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PrimeQA (unnormalized) Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(root_folder, results_folder, \"UNnormalized_scores.csv\"), \"w\") as outFile:\n",
    "    for setting in [\"All\", \"NoBB\"]:\n",
    "        print(f\"Results for \\\"{setting}\\\" setting\")\n",
    "        outFile.writelines(f\"Results for \\\"{setting}\\\" setting\"+\"\\n\")\n",
    "        print(f\"Passage, {'Question'.ljust(10)}, FScore, Exact Match\")\n",
    "        outFile.writelines(f\"Passage, {'Question'.ljust(10)}, FScore, Exact Match\"+\"\\n\")\n",
    "        for lang in [\"EN\", \"MSA\"]:\n",
    "            res_folder = os.path.join(root_folder, results_folder, setting, f'Results_{lang}')\n",
    "            all_f1 = []\n",
    "            all_exact = []\n",
    "            all_Qs = []\n",
    "            for p in sorted(os.listdir(res_folder)):\n",
    "                if p.startswith(\".DS\"):\n",
    "                    continue\n",
    "                # print(res_folder)\n",
    "                results = json.load(open(os.path.join(res_folder, p,\"eval_results.json\")))\n",
    "                if p not in [f'{lang}-P_MSA-Q', f'{lang}-P_EN-Q']:\n",
    "                    all_Qs.append(results['eval_samples'])\n",
    "                    all_f1.append(results['eval_f1'])\n",
    "                    all_exact.append(results['eval_exact_match'])\n",
    "                #     print(f\"{lang}\".ljust(6), f\", {dialects[p.split('_')[1].split('-')[0].lower()]}\".ljust(11), f\", {results['eval_f1']:.1f}\".ljust(7), f\", {results['eval_exact_match']:.1f} ,\")\n",
    "                # else:\n",
    "                print(f\"{lang}\".ljust(6), f\", {dialects[p.split('_')[1].split('-')[0].lower()]}\".ljust(11), f\", {results['eval_f1']:.1f}\".ljust(7), f\", {results['eval_exact_match']:.1f} ,\")\n",
    "                outFile.writelines(f\"{lang}\".ljust(6)+ f\", {dialects[p.split('_')[1].split('-')[0].lower()]}\".ljust(11)+ f\", {results['eval_f1']:.1f}\".ljust(7)+ f\", {results['eval_exact_match']:.1f} ,\"+\"\\n\")\n",
    "\n",
    "            # print(\"\\n\\nAll Qs, Avg F1, Avg Exact\")\n",
    "            # print(f\" & {sum(all_Qs):,}          & {np.average(all_f1):.1f} & {np.average(all_exact):.1f} \")\n",
    "            print(f\"{lang}\".ljust(6), f\", DA--Avg.\".ljust(11), f\", {np.average(all_f1):.1f}\".ljust(7), f\", {np.average(all_exact):.1f} ,\")\n",
    "            outFile.writelines(f\"{lang}\".ljust(6)+ f\", DA--Avg.\".ljust(11)+ f\", {np.average(all_f1):.1f}\".ljust(7)+ f\", {np.average(all_exact):.1f} ,\"+ str(sum(all_Qs))+ \"\\n\\n\")\n",
    "            \n",
    "            print(\"\\n\")\n",
    "        print(\"====================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalized Scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(root_folder, results_folder, \"Normalized_scores.csv\"), \"w\") as outFile:\n",
    "    for setting in [\"All\", \"NoBB\"]:        \n",
    "        res_folder = os.path.join(root_folder, results_folder, \"normalizedScore\")\n",
    "\n",
    "        print(f\"Results for \\\"{setting}\\\" setting\")\n",
    "        outFile.writelines(f\"Results for \\\"{setting}\\\" setting\"+\"\\n\")\n",
    "        print(f\"Passage, {'Question'.ljust(10)}, FScore, Exact Match\")\n",
    "        outFile.writelines(f\"Passage, {'Question'.ljust(10)}, FScore, Exact Match\"+\"\\n\")\n",
    "        for lang in [\"EN\", \"MSA\"]:\n",
    "            all_f1 = []\n",
    "            all_exact = []\n",
    "            all_Qs = []\n",
    "            results = {}\n",
    "            for p in sorted(glob.glob(os.path.join(f\"{res_folder}/{setting}\", \"RefPredPairs_Processed\", f\"{lang}*\"))):\n",
    "                # print(p)\n",
    "                # Read the csv file with sep = '\\t'\n",
    "                df = pd.read_csv(p, sep='\\t')\n",
    "                p = os.path.basename(p)\n",
    "                # print(p)\n",
    "                f1_mean = df['F1 Score_new'].mean() * 100\n",
    "                em_mean = df['Exact Score_new'].mean() * 100\n",
    "                if p not in [f'{lang}-P_MSA-Q', f'{lang}-P_EN-Q']:        \n",
    "            #         all_Qs.append(results['eval_samples'])\n",
    "                    all_f1.append(f1_mean)\n",
    "                    all_exact.append(em_mean)\n",
    "                print(f\"{lang}\".ljust(6), f\", {dialects[p.split('_')[1].split('-')[0].lower()]}\".ljust(11), f\", {f1_mean:.1f}\".ljust(7), f\", {em_mean:.1f} \")            \n",
    "                outFile.writelines(f\"{lang}\".ljust(6)+ f\", {dialects[p.split('_')[1].split('-')[0].lower()]}\".ljust(11)+ f\", {f1_mean:.1f}\".ljust(7)+ f\", {em_mean:.1f} \"+\"\\n\")\n",
    "\n",
    "            print(f\"{lang}\".ljust(6), f\", DA--Avg.\".ljust(11), f\", {np.average(all_f1):.1f}\".ljust(7), f\", {np.average(all_exact):.1f} \")\n",
    "            outFile.writelines(f\"{lang}\".ljust(6)+ f\", DA--Avg.\".ljust(11)+ f\", {np.average(all_f1):.1f}\".ljust(7)+ f\", {np.average(all_exact):.1f} \"+\"\\n\\n\")\n",
    "            \n",
    "            print(\"\\n\")\n",
    "        print(\"====================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DONE!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "primeqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
