{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Malik Altakrori, PhD\n",
    "# IBM Research\n",
    "# malik.altakrori@ibm.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AFTER COMPLETING ALL THE EXPERIMENTS from notebook 2, you can start with this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from primeqa.mitqa.metrics.evaluate import compute_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = \"<Provide the abs path to the repo>\"\n",
    "\n",
    "# you need to decide which experiment to run\n",
    "results_folder = \"Results\" # or [\"Results\", \"Results_Translated\"] for the translated dialectal questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function removes the unmatched quote punctuation as sometimes a generative model would generate one and stop (it is selecting a span)\n",
    "# We remove this extra quote to prevent it from messing up csv files that we will use\n",
    "def remove_extra_quote(pred, verbose=False):\n",
    "    if verbose:\n",
    "        print(f\"before: {pred}\")\n",
    "    processed_prediction = pred\n",
    "    last_loc = 0\n",
    "    even = 1 # odd and even (* -1)\n",
    "    for i, c in enumerate(processed_prediction):\n",
    "        if c == \"\\\"\":\n",
    "            even *= -1\n",
    "            last_loc = i\n",
    "\n",
    "    if even == -1 : # one does not match\n",
    "        processed_prediction = processed_prediction[0:last_loc] + processed_prediction[last_loc+1:]\n",
    "        \n",
    "    if verbose:\n",
    "        print(f\"After: {processed_prediction}\")\n",
    "    return processed_prediction\n",
    "\n",
    "pred = \"\\\"Troy\\\"\\\".\"\n",
    "if pred.count(\"\\\"\") % 2 == 1:\n",
    "    print(\"Caught a case\")\n",
    "    remove_extra_quote(pred, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the pairs for each setting/Lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we extract the refernce answer and the generated answer for each experiment, and calculate the unnormalized f1scores \n",
    "for setting in [\"All\", \"NoBB\"]:\n",
    "    os.makedirs(os.path.join(root_folder, results_folder, \"normalizedScore\", setting, \"RefPredPairs\"), exist_ok=True)\n",
    "\n",
    "    for lang in [\"EN\", \"MSA\"]:\n",
    "        full_path = os.path.join(root_folder, results_folder, setting, f\"Results_{lang}\")\n",
    "        for p in os.listdir(full_path):\n",
    "            if p.startswith(\".DS\"):\n",
    "                continue\n",
    "            print(f\"{full_path}/{p}\")    \n",
    "            with open(f\"{full_path}/{p}/eval_predictions_processed.json\", \"r\") as predsFile:\n",
    "                predsList = json.load(predsFile)            \n",
    "\n",
    "            with open(f\"{full_path}/{p}/eval_references.json\", \"r\") as refsFile:\n",
    "                refsList = json.load(refsFile)\n",
    "\n",
    "            with open(os.path.join(root_folder, results_folder, \"normalizedScore\",setting, \"RefPredPairs\",f\"{p}.tsv\"), \"w\") as outFile:\n",
    "                outFile.write(\"Reference\\tPredection\\tF1_score\\n\")\n",
    "                for r, p in zip(refsList, predsList):            \n",
    "                    f1 = compute_f1(a_gold=r['answers']['text'][0], a_pred=p['prediction_text'])\n",
    "\n",
    "                    processed_prediction = p['prediction_text']\n",
    "                    if processed_prediction.count(\"\\\"\") % 2 == 1:\n",
    "                        # print(\"Caught a case\")\n",
    "                        processed_prediction = remove_extra_quote(processed_prediction)\n",
    "\n",
    "                    outFile.write(f\"{r['answers']['text'][0]}\\t{processed_prediction}\\t{f1}\\n\")\n",
    "        #             break \n",
    "\n",
    "        #     break\n",
    "        # break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the script to run the code to calculate the normalized f1-score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_file_loc = os.path.join(root_folder, \"Notebooks\", \"f1_em_eval.py\")\n",
    "eval_file_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not results_folder.endswith(\"Translated\"):\n",
    "    out_ = \"original\"\n",
    "else:\n",
    "    out_ = \"translated\"\n",
    "\n",
    "with open(os.path.join(root_folder, \"Scripts\", out_, \"run_new_metric_all.sh\"), 'w') as outFile:\n",
    "    outFile.writelines(\"#!/usr/bin/env bash \\n\")\n",
    "    outFile.writelines(\"echo \\\"dont forget to activate your env\\\" \\n\\n\")\n",
    "\n",
    "    for setting in [\"All\", \"NoBB\"]:\n",
    "        input_folder = os.path.join(root_folder, results_folder, \"normalizedScore\", setting, \"RefPredPairs\")\n",
    "        output_folder = os.path.join(root_folder, results_folder, \"normalizedScore\", setting, \"RefPredPairs_Processed\" )\n",
    "\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        for p in os.listdir(input_folder):\n",
    "            input_file_path = os.path.join(input_folder, p)\n",
    "            output_file_path = os.path.join(output_folder, p)\n",
    "            lang_switch = p.split('-')[0]\n",
    "            lang = \"arabic\" if lang_switch == \"MSA\" else \"english\"\n",
    "            cmd = f\"python {eval_file_loc} --input_file {input_file_path} --output_file {output_file_path} --language {lang}\\n\"\n",
    "\n",
    "            print(cmd, end=\"\")\n",
    "            outFile.writelines(cmd)\n",
    "        #     break\n",
    "        # break"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "This will generate the file: \n",
    "Scripts/original/run_new_metric_all.sh that can be used to calculate all the new scores, or\n",
    "Scripts/translated/run_new_metric_all.sh\n",
    "\n",
    "DONT FORGET TO RUN YOUR env BEFORE RUNNING THE SCRIPT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "primeqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
